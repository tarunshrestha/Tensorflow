{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23ba4eb9-f4c3-4cad-9447-01880db1281b",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24fda05c-0212-452b-a9a6-2dffb65ed115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO\n"
     ]
    }
   ],
   "source": [
    "print(\"HELLO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f2dad49-c071-4ffd-8892-f9ec7b48979f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 15:04:46.561434: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-27 15:04:46.567680: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-27 15:04:46.580524: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-27 15:04:46.606270: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-27 15:04:46.614306: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-27 15:04:46.633634: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-27 15:04:47.759086: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf \n",
    "import os\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "912b4770-1c9a-4827-af94-0cc7241589f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic \n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90bef81d-0432-430c-9e13-2b409ab25894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
    "                             mp_drawing.DrawingSpec(color=(245, 117, 76), thickness=1, circle_radius=1),\n",
    "                             mp_drawing.DrawingSpec(color=(245, 44, 250), thickness=1, circle_radius=1)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44e6ef4f-a5eb-4589-bb16-bfaae88b7737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Color conversion BGR to RGB\n",
    "    image.flags.writeable = False  # Image is no longer writeable\n",
    "    results = model.process(image)  # Make prediction\n",
    "    image.flags.writeable = True  # Image writeable\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # Color conversion RGB to BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4cef134-cd38-45c7-9ac3-7e452e7f2a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() \\\n",
    "                if results.face_landmarks else np.zeros(468*3)\n",
    "    return np.concatenate([face])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ebb2075-037a-427b-b96b-9410856b1b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1727429345.602614   15468 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1727429345.603942   15969 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 23.2.1-1ubuntu3.1~22.04.2), renderer: Mesa Intel(R) HD Graphics 630 (KBL GT2)\n",
      "W0000 00:00:1727429345.685373   15960 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1727429345.722024   15959 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1727429345.723498   15962 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1727429345.723505   15961 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1727429345.723849   15966 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1727429345.728080   15962 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1727429345.734039   15965 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1727429345.743673   15960 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "# Access mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cam.isOpened():\n",
    "    \n",
    "        # Read feed\n",
    "        ret, frame = cam.read()\n",
    "        \n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        # print(result)\n",
    "\n",
    "        # Draw Landmarks\n",
    "        draw_landmarks(image, results)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow(\"OpenCV Feed\", image)\n",
    "        \n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10)& 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the webcam and close all OpenCV windows\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3ce9165-0bdd-44bf-842e-000216112bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b50fe44d-12d6-4f74-81ec-04035d32099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_landmarks(frame, results)\n",
    "# plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0c2599a-b837-48fd-8df7-1dfcc992d3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results.face_landmarks.landmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e792a05-4f77-4e75-b8f7-4af07ac626f6",
   "metadata": {},
   "source": [
    "# Folder setup for data storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b7502fc-cd14-4f21-9db1-c51efdb402f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data\n",
    "DATA_PATH = os.path.join(\"FACE_DATA\")\n",
    "\n",
    "# Actions to detect\n",
    "actions = np.array([\"happy\", \"sad\", 'angry'])\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 30\n",
    "\n",
    "# Frames \n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24a7f7e4-f9ed-4a69-92b4-a20ce604a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make directory for each video and frames\n",
    "# for action in actions:\n",
    "#     for sequence in range(no_sequences):\n",
    "#         try:\n",
    "#             os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "#         except:\n",
    "#             pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "232b8ec2-6d89-4851-8b5f-98e4928b67bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727428789.545801   15468 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1727428789.563887   15570 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 23.2.1-1ubuntu3.1~22.04.2), renderer: Mesa Intel(R) HD Graphics 630 (KBL GT2)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1727428789.678795   15554 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1727428789.724015   15558 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1727428789.730026   15553 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1727428789.731164   15556 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1727428789.731903   15560 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1727428789.741779   15558 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1727428789.759344   15554 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1727428789.760629   15559 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1727428791.002495   15553 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "/home/tarun/Documents/Tensorflow/venv/lib/python3.10/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "Warning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway.\n"
     ]
    }
   ],
   "source": [
    "# cam = cv2.VideoCapture(0)\n",
    "\n",
    "# if not cam.isOpened():\n",
    "#     print(\"Error: Could not open webcam.\")\n",
    "#     exit()\n",
    "    \n",
    "# # Access mediapipe model\n",
    "# with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "#     # while cam.isOpened():\n",
    "\n",
    "#     # Loop through actions\n",
    "#     for action in actions:\n",
    "#         # Loop through video/ sequences\n",
    "#         for sequence in range(no_sequences):\n",
    "#             # Loop through frames\n",
    "#             for frame_num in range(sequence_length):\n",
    "                \n",
    "#                 # Read feed\n",
    "#                 ret, frame = cam.read()\n",
    "                \n",
    "#                 # Make detections\n",
    "#                 image, results = mediapipe_detection(frame, holistic)\n",
    "#                 # print(result)\n",
    "        \n",
    "#                 # Draw Landmarks\n",
    "#                 draw_landmarks(image, results)\n",
    "\n",
    "#                 # Apply collection logic\n",
    "#                 if frame_num == 0:\n",
    "#                     cv2.putText(image, \"Starting collection\", (120, 200),\n",
    "#                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "#                     cv2.putText(image, \"Collecting frames for {} Video Number {}\".format(action, sequence), (15, 12),\n",
    "#                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "#                     cv2.waitKey(2000)\n",
    "#                 else:\n",
    "#                     cv2.putText(image, \"Collecting frames for {} Video Number {}\".format(action, sequence), (15, 12),\n",
    "#                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "#                 # New Export key points    \n",
    "#                 keypoints = extract_keypoints(results)\n",
    "#                 npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "#                 np.save(npy_path, keypoints)\n",
    "                \n",
    "#                 # Show to screen\n",
    "#                 cv2.imshow(\"OpenCV Feed\", image)\n",
    "            \n",
    "#                 # Break gracefully\n",
    "#                 if cv2.waitKey(10)& 0xFF == ord('q'):\n",
    "#                     break\n",
    "\n",
    "#     # Release the webcam and close all OpenCV windows\n",
    "#     cam.release()\n",
    "#     cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
